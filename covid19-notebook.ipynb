{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c36cb9e0",
   "metadata": {},
   "source": [
    "# Covid 19 Identification with Chest X Ray\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552370cc",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "### Total Covid19 Positive images 4100\n",
    "### Total Covid19 Negative images ( Pneumonia + Normal ) 5362\n",
    "\n",
    "\n",
    "## Credits and Links to open source dataset\n",
    "\n",
    "1. [Covid Chest X Ray Dataset](https://github.com/ieee8023/covid-chestxray-dataset)\n",
    "2. [Pneumonia dataset by Praveen](https://www.kaggle.com/praveengovi/coronahack-chest-xraydataset)\n",
    "3. [COVID19 chest XRAY analysis by SAIMANASA_C](https://www.kaggle.com/code/saimanasachadalavada/covid19-chest-xray-analysis/data)\n",
    "4. [COVID19 with Pneumonia and Normal Chest Xray(PA) Dataset by AMANULLAH ASRAF](https://www.kaggle.com/datasets/amanullahasraf/covid19-pneumonia-normal-chest-xray-pa-dataset)\n",
    "5. [HASH_Directors - Covid19 by AMRUTH AMBRISH K](https://www.kaggle.com/code/amruthambrish/hash-directors-covid19/data)\n",
    "6. [RICORD COVID-19 X-ray positive tests by RADDAR](https://www.kaggle.com/datasets/raddar/ricord-covid19-xray-positive-tests)\n",
    "\n",
    "\n",
    "\n",
    "Created By Lalith Kahatapitiya at [PGIS](http://www.pgis.pdn.ac.lk/) - University of Peradeniya\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e34ff40",
   "metadata": {},
   "source": [
    "   ### Required phython packages and libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79281d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Platform: macOS-12.6-arm64-arm-64bit\n",
      "Python 3.9.13 | packaged by conda-forge | (main, May 27 2022, 17:00:33) \n",
      "[Clang 13.0.1 ]\n",
      "\n",
      "Tensor Flow Version: 2.10.0\n",
      "Keras Version: 2.10.0\n",
      "Pandas 1.5.0\n",
      "Scikit-Learn 1.1.2\n",
      "Numpy 1.23.2\n",
      "Open CV 4.6.0\n",
      "\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "import platform\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "import tensorflow.keras\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "\n",
    "print(f\"Python Platform: {platform.platform()}\")\n",
    "print(f\"Python {sys.version}\")\n",
    "print()\n",
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {tensorflow.keras.__version__}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "print(f\"Numpy {np.__version__}\")\n",
    "print(f\"Open CV {cv2.__version__}\")\n",
    "print()\n",
    "gpu = len(tf.config.list_physical_devices('GPU'))>0\n",
    "print(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3080153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', 'NORMAL copy', 'NORMAL', 'COVID']\n"
     ]
    }
   ],
   "source": [
    "# image data set_path mapping check folder names \n",
    "dataset_path='TRAINING'\n",
    "\n",
    "categories=os.listdir(dataset_path)\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76d9eb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NORMAL copy': 0, 'NORMAL': 1, 'COVID': 2}\n",
      "['NORMAL copy', 'NORMAL', 'COVID']\n",
      "[0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "# image data load to dictionary \n",
    "\n",
    "categories_all=os.listdir(dataset_path)\n",
    "categories=[]\n",
    "for category in categories_all:\n",
    "    if(category != \".DS_Store\"):\n",
    "        categories.append(category)\n",
    "        \n",
    "labels=[i for i in range(len(categories))]\n",
    "\n",
    "label_dict=dict(zip(categories,labels)) #empty dictionary\n",
    "\n",
    "print(label_dict)\n",
    "print(categories)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29209e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception: OpenCV(4.6.0) /Users/xperience/actions-runner/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
      "\n",
      "Exception: OpenCV(4.6.0) /Users/xperience/actions-runner/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
      "\n",
      "Exception: OpenCV(4.6.0) /Users/xperience/actions-runner/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "img_size=224\n",
    "data=[]\n",
    "target=[]\n",
    "\n",
    "\n",
    "for category in categories:\n",
    "    folder_path=os.path.join(dataset_path,category)\n",
    "    img_names=os.listdir(folder_path)\n",
    "\n",
    "    for img_name in img_names:\n",
    "        img_path=os.path.join(folder_path,img_name)\n",
    "        img=cv2.imread(img_path)\n",
    "\n",
    "        try:\n",
    "            gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)           \n",
    "                #Coverting the image into gray scale\n",
    "            resized=cv2.resize(gray,(img_size,img_size))\n",
    "                #resizing the gray scale into 224x224, since we need a fixed common size for all the images in the dataset\n",
    "            data.append(resized)\n",
    "            target.append(label_dict[category])\n",
    "                #appending the image and the label(categorized) into the list (dataset)\n",
    "\n",
    "        except Exception as e:\n",
    "             print('Exception:',e)\n",
    "                #if any exception rasied, the exception will be printed here. And pass to the next image  \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c721509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape : (51, 224, 224, 1) \n",
      "Target shape : (51,) \n",
      "New Target shape : (51, 3)\n"
     ]
    }
   ],
   "source": [
    "data=np.array(data)/255.0\n",
    "data=np.reshape(data,(data.shape[0],img_size,img_size,1))\n",
    "target=np.array(target)\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "new_target=np_utils.to_categorical(target)\n",
    "\n",
    "# standard binary file format in NumPy for persisting a single arbitrary NumPy array on disk.\n",
    "np.save('data',data)\n",
    "np.save('target',new_target)\n",
    "\n",
    "print(f\"Data shape : {data.shape} \\nTarget shape : {target.shape} \\nNew Target shape : {new_target.shape}\")\n",
    "\n",
    "\n",
    "# target\n",
    "# print()\n",
    "# new_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55a07035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51, 224, 224, 1)\n",
      "(51, 3)\n",
      "Metal device set to: Apple M1 Pro\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " model (Functional)          (None, 224, 224, 256)     896       \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 222, 222, 64)      147520    \n",
      "                                                                 \n",
      " activation (Activation)     (None, 222, 222, 64)      0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 111, 111, 64)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 109, 109, 32)      18464     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 109, 109, 32)      0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 54, 54, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 93312)             0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 93312)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               11944064  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,119,330\n",
      "Trainable params: 12,119,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-09 08:38:16.938136: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-10-09 08:38:16.938250: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Start model creation:{datetime.datetime.now()}\")\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D,Activation,MaxPooling2D\n",
    "from keras.utils import normalize\n",
    "from keras.layers import Concatenate\n",
    "from keras import Input\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "data=np.load('data.npy')\n",
    "target=np.load('target.npy')\n",
    "\n",
    "print(data.shape)\n",
    "print(target.shape)\n",
    "\n",
    "\n",
    "input_shape=data.shape[1:] #50,50,1\n",
    "inp=Input(shape=input_shape)\n",
    "convs=[]\n",
    "\n",
    "parrallel_kernels=[3,5,7]\n",
    "\n",
    "for k in range(len(parrallel_kernels)):\n",
    "    if(k !=0):\n",
    "        conv = Conv2D(128, kernel_size = k,padding = 'same' ,activation='relu')(inp)\n",
    "\n",
    "        convs.append(conv)\n",
    "\n",
    "out = Concatenate()(convs)\n",
    "conv_model = Model(inp, out)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(conv_model)\n",
    "\n",
    "model.add(Conv2D(64,(3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(32,(3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2,input_dim=128,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "print(f\"End model creation:{datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8e8f259",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data,test_data,train_target,test_target=train_test_split(data,target,test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57e78c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 224, 224, 1)\n",
      "(45, 3)\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-09 08:38:17.193159: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/lalithk90/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/lalithk90/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/lalithk90/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/lalithk90/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/training.py\", line 994, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Users/lalithk90/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"/Users/lalithk90/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/Users/lalithk90/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/Users/lalithk90/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/losses.py\", line 272, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/Users/lalithk90/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/losses.py\", line 1990, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/Users/lalithk90/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/backend.py\", line 5529, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 3) and (None, 2) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_data\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_target\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 10\u001b[0m history\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfit(train_data,\n\u001b[1;32m     11\u001b[0m                   train_target,\n\u001b[1;32m     12\u001b[0m                   epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m     13\u001b[0m                   callbacks\u001b[38;5;241m=\u001b[39m[checkpoint],\n\u001b[1;32m     14\u001b[0m                   validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/4s/_8s89ltj0jd8c84b01jt_6640000gn/T/__autograph_generated_file7tl52a14.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/lalithk90/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/lalithk90/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/lalithk90/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/lalithk90/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/training.py\", line 994, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Users/lalithk90/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"/Users/lalithk90/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/Users/lalithk90/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/Users/lalithk90/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/losses.py\", line 272, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/Users/lalithk90/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/losses.py\", line 1990, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/Users/lalithk90/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/keras/backend.py\", line 5529, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 3) and (None, 2) are incompatible\n"
     ]
    }
   ],
   "source": [
    "print(f\"Start checkpoint creation:{datetime.datetime.now()}\")\n",
    "checkpoint = ModelCheckpoint('model-{epoch:03d}.model',\n",
    "                             monitor='val_loss',\n",
    "                             verbose=0,\n",
    "                             save_best_only=True,\n",
    "                             mode='auto')\n",
    "\n",
    "\n",
    "history=model.fit(train_data,\n",
    "                  train_target,\n",
    "                  epochs=20,\n",
    "                  callbacks=[checkpoint],\n",
    "                  validation_split=0.1)\n",
    "print(f\"End checkpoint creation:{datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ec67a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# floting values in to graph\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'],'r',label='training loss')\n",
    "plt.plot(history.history['val_loss'],label='validation loss')\n",
    "plt.xlabel('# epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba42503",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'],'r',label='training accuracy')\n",
    "plt.plot(history.history['val_accuracy'],label='validation accuracy')\n",
    "plt.xlabel('# epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc73f111",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.evaluate(test_data,test_target))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
